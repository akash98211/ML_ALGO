{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HhRbmw6kip0a"
   },
   "outputs": [],
   "source": [
    "#1 What is Linear regression?\n",
    "\n",
    "'''Linear Regression is a supervised machine learning technique use to predict \n",
    "values of Target columns by finding linear relationship between dependant and independant variable'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QonIR9p8jTHo"
   },
   "outputs": [],
   "source": [
    "#2  How do you represent a simple linear regression?\n",
    "\n",
    "'''Simple linear regression has one independant variable and one dependant variable, \n",
    "equation is y=mx+C where m is the slope associated with independant variable and c is an intercept of best fit line'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G9I80eL_j3mM"
   },
   "outputs": [],
   "source": [
    "#3 What is multiple linear regression?\n",
    "\n",
    "'''Multiple linear regression has more than one independant variable and one dependant variable, \n",
    "equation is y=m1x1+m2x3+....+mnxn+C where m is the slope associated with independant variable and c is an intercept of best fit line'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F9EEc4XlkKB1"
   },
   "outputs": [],
   "source": [
    "#4 What are the assumptions made in the Linear regression model?\n",
    "\n",
    "# Assupmtions are as follows\n",
    "\n",
    "'''\n",
    "1. There has to be a linear relation of each independant variable with dependant variable\n",
    "\n",
    "2. There shouldn't be a multicolinearity between independant varibales\n",
    "\n",
    "3. Normality of residual\n",
    "\n",
    "4. Homoscedascticity\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fizgn37wlluk"
   },
   "outputs": [],
   "source": [
    "#5 What if these assumptions get violated?\n",
    "\n",
    "'''Our Model might get overfitted or underfitted'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w48UmkQ_pglc"
   },
   "outputs": [],
   "source": [
    "#6 What is the assumption of homoscedasticity?\n",
    "\n",
    "'''Homoscedasticity means the variance for error term for all independent variable should be equal'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "apGRvQ3Spw9z"
   },
   "outputs": [],
   "source": [
    "#7 What is the assumption of normality?\n",
    "\n",
    "'''Normality of residual means the mean value of the residual should be nearly equal to 0'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a_yvc2m-qMu1"
   },
   "outputs": [],
   "source": [
    "#8 How to prevent heteroscedasticity?\n",
    "\n",
    "'''Detect and Handle outliers, because this algorithm is outlier sensetive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eLCHjqP42MLh"
   },
   "outputs": [],
   "source": [
    "#9 What does multicollinearity mean?\n",
    "\n",
    "'''Multicolinearity meaning is one of the independent variable is having linear relation with other independent variable,\n",
    "if x1 is increasing x2 also increasing'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VwXaOFUU2fb-"
   },
   "outputs": [],
   "source": [
    "#10 What are feature selection and feature scaling?\n",
    "\n",
    "'''Feature selection is nothing but selecting x and y for model building and feature \n",
    "scaling means applying Normalization or Standardization on features to scale down them in single range'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XkveB9jQ3A__"
   },
   "outputs": [],
   "source": [
    "#11 How to find the best fit line in a linear regression model?\n",
    "\n",
    "'''BFL is nothing but which has low bias and variance and High training and \n",
    "testing accuracy. Passes through maximum data points and have optimum c nad m values'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p0vfKGJf3ZiW"
   },
   "outputs": [],
   "source": [
    "#12 Why do we square the error instead of using modulus?\n",
    "\n",
    "'''If we use absolute value then that equation is not differentiable when we calculate optimum values of \n",
    "m and c using gradient descent, but if we use squared terms then we can differentiate that equation to find min or maximum value'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KRaF3sAS9NyX"
   },
   "outputs": [],
   "source": [
    "#13 What are techniques adopted to find the slope and the intercept of the linear regression line which best fits the model?\n",
    "\n",
    "'''Gradient descent is use to find the optimum values of m and c, \n",
    "it differenctiate cost function i.e MSE with respect to m and c and calculate it. In that alpha is used as Learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6XCwhZT9kd2d"
   },
   "outputs": [],
   "source": [
    "#14 What is cost Function in Linear Regression?\n",
    "\n",
    "'''MSE Mean Squarred Error is the Cost Function of Linear Regression, Sum(Ya-Yp)^2/n'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VGrtXM4ik0w5"
   },
   "outputs": [],
   "source": [
    "#15 briefly explain gradient descent algorithm\n",
    "\n",
    "'''\n",
    "Gradient descent is use to find the optimum values of m and c, heance it Gives Best Fit Line i.e best model.\n",
    "It differenctiate costs function i.e MSE with respect to m and c and calculate best values of m and c. \n",
    "In that alpha is used as Learning rate and Learning step will be each step and that will decrese at each iteration.\n",
    "Mnew = Mold - alpha*derivation(MSE) w.r.t 'm' \n",
    "Cnew = Cold - alpha*derivation(MSE) w.r.t 'c' \n",
    "This is how it will Keep on finding m and c values until\n",
    "iterations = 10000\n",
    "epsilon (sqrt.(Mnew^2-Mold^2))<0.001\n",
    "either of these conditions are getting satisfied\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QwHcHVDsougR"
   },
   "outputs": [],
   "source": [
    "#16 How to evaluate regression models?\n",
    "\n",
    "'''We generally evaluate regression model based on following metrics:\n",
    "1. r2_score\n",
    "2. MSE\n",
    "3. RMSE\n",
    "4. MAE\n",
    "5. model score\n",
    "\n",
    "these values we calculate for both Train and Test dataset and see if our model is optimal or overfitted or underfitted'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z4CNNrrVpNBf"
   },
   "outputs": [],
   "source": [
    "#17 Which evaluation technique should you prefer to use for data having a lot of outliers in it?\n",
    "\n",
    "'''Mean Absolute Error(MAE) is preferable to use for data having too many outliers in it because MAE is robust to outliers \n",
    "whereas MSE and RMSE are very susceptible to outliers and starts penalizing the outliers by squaring the residuals'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L3ANosJJqW5w"
   },
   "outputs": [],
   "source": [
    "#18 What is residual? How is it computed?\n",
    "\n",
    "'''Residual is also called as Error Term which is difference between actual and predicted terget column value'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D85_yxBXrI0A"
   },
   "outputs": [],
   "source": [
    "#19 What are SSE, SSR, and SST? and What is the relationship between them?\n",
    "\n",
    "'''\n",
    "SSE = Sum(Ya-Yp)^2/n\n",
    "\n",
    "SSR = Sum(Yp-Ymean)^2/n\n",
    "\n",
    "SST = SSE = Sum(Ya-Ymean)^2/n\n",
    "\n",
    "Coeff of determination,r_2 score :  1 - (SSE/SST)\n",
    "\n",
    "when Ya = Yp, r_2 score is 1 Good model\n",
    "\n",
    "when Ymean = Yp, r_2 score is 0 Bad Model\n",
    "''' \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qkAFId69vN__"
   },
   "outputs": [],
   "source": [
    "#20 Whatâ€™s the intuition behind R-Squared?\n",
    "\n",
    "'''\n",
    "Coeff of determination,r_2 score :  1 - (SSE/SST)\n",
    "\n",
    "when Ya = Yp, r_2 score is 1 Good model\n",
    "\n",
    "when Ymean = Yp, r_2 score is 0 Bad Model\n",
    "\n",
    "The r_2 value has one drawback, the value gets increse even if we add bad feature in our dataset. So we use adjusted_r2 score\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pF8oQFR-vtcV"
   },
   "outputs": [],
   "source": [
    "#21 What does the coefficient of determination explain?\n",
    "\n",
    "'''Coefficient of determination explains the goodness of bestfit line'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wPAwVjEFwsrH"
   },
   "outputs": [],
   "source": [
    "#22 Can R2 be negative?\n",
    "\n",
    "'''Note that it is possible to get a negative R-square for equations that do not contain a constant term.\n",
    "Because R-square is defined as the proportion of variance explained by the fit, \n",
    "if the fit is actually worse than just fitting a horizontal line then R-square is negative.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4z2B3zNDwzDE"
   },
   "outputs": [],
   "source": [
    "#23 What are the flaws in R-squared?\n",
    "\n",
    "'''The r_2 value has one drawback, the value gets increse even if we add bad feature in our dataset. So we use adjusted_r2 score'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yvuhMpGEw46l"
   },
   "outputs": [],
   "source": [
    "#24 What is adjusted R2?\n",
    "\n",
    "'''Adjusted r2 is always less that r2 score and formula is 1- (1-r2)(N-1) / (N-P-1) .\n",
    " It gives less value if we are adding any bad feature in our dataset and it gives higher value if we are adding good feature in dataset'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5lxZzcSdiVfI"
   },
   "outputs": [],
   "source": [
    "#25 What is the Coefficient of Correlation: Definition, Formula\n",
    "\n",
    "'''\n",
    "Coefficient of Correlation, R: It shows how each variable are correlated with each other. If the correlation is positive then value will be grater than zero, \n",
    "and 1 is for ideal positive correlation(max) and -1 for negative correlation(max).\n",
    "\n",
    "If correlation is greater than 0.7 then it said to be a very good positively correlated variables and if they are less than -0.7 towards -1\n",
    "then they also called as very good neagatively correlated variables.\n",
    "\n",
    "Formula is, Variance(x,y)/std(x)std(y)\n",
    "\n",
    "sum.(Xi - Xmean)(Yi - Ymean)/sum.sqrt.((Xi - Xmean)^2.(Yi - Ymean)^2)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SLYNNcsytCLk"
   },
   "outputs": [],
   "source": [
    "#26 What is the relationship between R-Squared and Adjusted R-Squared?\n",
    "\n",
    "'''R-Squared value is always greater than or equal to Adjusted R-Squared'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yZgDEodRthla"
   },
   "outputs": [],
   "source": [
    "#27 What is the difference between overfitting and underfitting?\n",
    "\n",
    "'''\n",
    "Overfitting: When the Training Accuracy is more and Testing Accuracy is less then model is overfitted. The Variance is more in overfitting \n",
    "Model and Bias is also low\n",
    "\n",
    "Underfitting: When the Training and Testing both accuracies are low that time Variance is low but Bias is high and the model is underfitted\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qXB4sEr1uEJ9"
   },
   "outputs": [],
   "source": [
    "#28 How to identify if the model is overfitting or underfitting?\n",
    "\n",
    "'''\n",
    "When the Training Accuracy is more and Testing Accuracy is less then model is overfitted and When the Training Accuracy \n",
    "is less and Testing Accuracy is less then model is underfitted\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AzDf4jIYuTzi"
   },
   "outputs": [],
   "source": [
    "#29 How to interpret a Q-Q plot in a Linear regression model?\n",
    "\n",
    "'''If all the points are lying very near to the linear line approximately on the line then its said to be a normal distribution of residual'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k-zu95gZur5v"
   },
   "outputs": [],
   "source": [
    "#30 What are the advantages and disadvantages of Linear Regression?\n",
    "\n",
    "'''\n",
    "Advantages: \n",
    "1. Easy to implement\n",
    "2. Easy to understand\n",
    "3. less complexity to compared to other algorithms.\n",
    "4. Linear Regression is susceptible to over-fitting but it can be avoided using \n",
    "   some dimensionality reduction techniques, regularization (L1 and L2) techniques and cross-validation.\n",
    "\n",
    "Disadvantages:\n",
    "1. Sensitive to outliers\n",
    "2. Parametric, comply with all the assumption\n",
    "3. Need to handle null values\n",
    "4. linear regression is not a complete description of relationships among variables.\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g74-scUPyF2D"
   },
   "outputs": [],
   "source": [
    "#31 What is the use of regularisation? Explain L1 and L2 regularisations.\n",
    "\n",
    "'''\n",
    "Regularization is use when we get overfitted model for Linear and Logistic Regression\n",
    "\n",
    "It is use to reduce the multicolinearity of the independent variable, because overfitting generated due to multicolinearity\n",
    "\n",
    "Here we reduce Training Accuracy and Increase Testing Accuracy by lowering the m value that is slope.\n",
    "Here we just add this term, lambda*(slope^2) in linear and logistic regression cost function in Ridge(L2) and in Lasso(L1)\n",
    "we use lambda*|slope| instead of square. For more features we prefer L1 i.e Lasso.\n",
    "\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "68_Mihir_NaiK_Linear_Regression_Assignment.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
